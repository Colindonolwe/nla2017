{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1: Floating-point arithmetic, vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Syllabus\n",
    "**Week 1:** floating point, vector norms, matrix multiplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today \n",
    "- Fixed/floating point arithmetic;\n",
    "- Concept of **backward** and **forward** stability of algorithms\n",
    "- How to measure accuracy: vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "\n",
    "Real numbers represent quantities: probabilities, velocities, masses, ...\n",
    "\n",
    "It is important to know, how they are represented in the computer (which only knows about bits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed point\n",
    "\n",
    "The most straightforward format for the representation of real numbers is **fixed point** representation,\n",
    "\n",
    "also known as **Qm.n** format.\n",
    "\n",
    "A **Qm.n** number is in the range $[-(2^m), 2^m - 2^{-n}]$, with resolution $2^{-n}$.\n",
    "\n",
    "Total storage is $m + n + 1$ bits.\n",
    "\n",
    "The range of numbers represented is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of numbers\n",
    "The numbers in computer memory are typically represented as **floating point numbers** \n",
    "\n",
    "A floating point number is represented as  \n",
    "\n",
    "$$\\textrm{number} = \\textrm{significand} \\times \\textrm{base}^{\\textrm{exponent}},$$\n",
    "\n",
    "where *significand* is integer, *base* is positive integer  and *exponent* is integer (can be negative), i.e.\n",
    "\n",
    "$$ 1.2 = 12 \\cdot 10^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed vs Floating\n",
    "\n",
    "**Q**: What are the advantages/disadvantages of the fixed and floating points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A**:  In most cases, they work just fine.\n",
    "\n",
    "However, fixed point represents numbers within specified range and controls **absolute** accuracy.\n",
    "\n",
    "Floating point represent numbers with **relative** accuracy, and is suitable for the case when numbers in the computations have varying scale \n",
    "\n",
    "(i.e., $10^{-1}$ and $10^{5}$).\n",
    "\n",
    "In practice, if speed is of no concern, use float32 or float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IEEE 754\n",
    "In modern computers, the floating point representation is controlled by [IEEE 754 standard](https://en.wikipedia.org/wiki/IEEE_floating_point) which was published in **1985** and before that point different computers behaved differently with floating point numbers. \n",
    "\n",
    "IEEE 754 has:\n",
    "- Floating point representation (as described above), $(-1)^s \\times c \\times b^q$.\n",
    "- Two infinities, $+\\infty$ and $-\\infty$\n",
    "- Two kinds of **NaN**: a quiet NaN (**qNaN**) and signalling NaN (**sNaN**) \n",
    "- Rules for **rounding**\n",
    "- Rules for $\\frac{0}{0}, \\frac{1}{-0}, \\ldots$\n",
    "\n",
    "$ 0 \\leq c \\leq b^p - 1, \\quad 1 - emax \\leq q + p - 1 \\leq emax$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The two most common format, single & double\n",
    "\n",
    "The two most common format, called **binary32** and **binary64** (called also **single** and **double** formats).\n",
    "\n",
    "| Name | Common Name | Base | Digits | Emin | Emax |\n",
    "|------|----------|----------|-------|------|------|\n",
    "|binary32| single precision | 2 | 11 | -14 | + 15 |  \n",
    "|binary64| double precision | 2 | 24 | -126 | + 127 |  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy and memory\n",
    "The **relative accuracy** of single precision is $10^{-7}-10^{-8}$,  \n",
    "\n",
    "while for double precision is $10^{-14}-10^{-16}$.\n",
    "\n",
    "<font color='red'> Crucial note 1: </font> A **float32** takes **4 bytes**, **float64**, or double precision, takes **8 bytes.**\n",
    "\n",
    "<font color='red'> Crucial note 2: </font> These are the only two floating point-types supported in hardware.\n",
    "\n",
    "<font color='red'> Crucial note 3: </font> You should use **double precision** in CSE and **float** on GPU/Data Science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some demo (for division accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1040364727377892\n",
      "-5.96046e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "#c = random.random()\n",
    "#print(c)\n",
    "c = np.float32(0.925924589693)\n",
    "a = np.float32(8.9)\n",
    "b = np.float32(c / a)\n",
    "print('{0:10.16f}'.format(b))\n",
    "print a * b - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0000001468220603\n"
     ]
    }
   ],
   "source": [
    "#a = np.array(1.585858585887575775757575e-5, dtype=np.float)\n",
    "a = np.array(5.0, dtype=np.float32)\n",
    "b = np.sqrt(a)\n",
    "print('{0:10.16f}'.format(b ** 2 - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(2.28827272710, dtype=np.float32)\n",
    "b = np.exp(a)\n",
    "print np.log(b) - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- For some values the inverse functions give exact answers\n",
    "- The relative accuracy should be kept due to the IEEE standard.\n",
    "- Does not hold for many modern GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss of significance\n",
    "\n",
    "Many operations lead to the loss of digits [loss of significance](https://en.wikipedia.org/wiki/Loss_of_significance)\n",
    "\n",
    "\n",
    "For example, it is a bad idea to subtract two big numbers that are close, the difference will have fewer correct digits.\n",
    "\n",
    "This is related to algorithms and their properties (forward/backward stability), which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summation algorithm\n",
    "\n",
    "However, the rounding errors can depend on the algorithm. \n",
    "\n",
    "Consider the simplest problem: given $n$ floating point numbers $x_1, \\ldots, x_n$  \n",
    "\n",
    "compute their sum\n",
    "\n",
    "$$S = \\sum_{i=1}^n x_i = x_1 + \\ldots + x_n.$$\n",
    "\n",
    "The simplest algorithm is to add one-by-one. \n",
    "\n",
    "What is the actual error for such algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive algorithm\n",
    "\n",
    "Naive algorithm adds numbers one-by-one, \n",
    "\n",
    "$$y_1 = x_1, \\quad y_2 = y_1 + x_2, \\quad y_3 = y_2 + x_3, \\ldots.$$\n",
    "\n",
    "The worst-case error is then proportional to $\\mathcal{O}(n)$, while **mean-squared** error is $\\mathcal{O}(\\sqrt{n})$.\n",
    "\n",
    "The **Kahan algorithm** gives the worst-case error bound $\\mathcal{O}(1)$ (i.e., independent of $n$).  \n",
    "\n",
    "<font color='red'> Can you find the $\\mathcal{O}(\\log n)$ algorithm? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kahan summation\n",
    "The following algorithm gives $2 \\varepsilon + \\mathcal{O}(n \\varepsilon^2)$ error, where $\\varepsilon$ is the machine precision.\n",
    "```python\n",
    "s = 0\n",
    "c = 0\n",
    "for i in range(len(x)):\n",
    "    y = x[i] - c\n",
    "    t = s + y\n",
    "    c = (t - s) - y\n",
    "    s = t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in sum: 1.9e-04, \n",
      "kahan: -1.3e-07, \n",
      "dumb_sum: -1.0e-02. \n"
     ]
    }
   ],
   "source": [
    "n = 10 ** 8\n",
    "sm = 1e-10\n",
    "x = np.ones(n, dtype=np.float32) * sm\n",
    "x[0] = 1.0\n",
    "true_sum = 1.0 + (n - 1)*sm\n",
    "approx_sum = np.sum(x)\n",
    "math_sum = math.fsum(x)\n",
    "\n",
    "from numba import jit\n",
    "@jit\n",
    "def dumb_sum2(x):\n",
    "    s = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        s = s + x[i]\n",
    "    return s\n",
    "@jit\n",
    "def kahan_sum(x):\n",
    "    s = np.float32(0.0)\n",
    "    c = np.float32(0.0)\n",
    "    for i in range(len(x)):\n",
    "        y = x[i] - c\n",
    "        t = s + y\n",
    "        c = (t - s) - y\n",
    "        s = t\n",
    "    return s\n",
    "k_sum = kahan_sum(x)\n",
    "d_sum = dumb_sum2(x)\n",
    "print('Error in sum: {0:3.1e}, \\nkahan: {1:3.1e}, \\ndumb_sum: {2:3.1e}. '.format(approx_sum - true_sum, k_sum - true_sum, d_sum - true_sum, math_sum - true_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print math.fsum([1, 1e20, 1, -1e20]), np.sum([1, 1e20, 1, -1e20]), 1 + 1e20 + 1 - 1e20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of floating-point \n",
    "You should be really careful with floating point, since it may give you incorrect answers due to rounding-off errors.\n",
    "\n",
    "For many standard algorithms, the stability is well-understood and problems can be easily detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "In NLA we typically work not with **numbers**, but with **vectors**. \n",
    "\n",
    "Recall that a vector in a fixed basis of size $n$ can be represented as a 1D array with $n$ numbers. \n",
    "\n",
    "Typically, it is considered as an $n \\times 1$ matrix (**column vector**).\n",
    "\n",
    "**Example:** \n",
    "Polynomials with degree $\\leq n$ form a linear space. \n",
    "Polynomial $ x^3 - 2x^2 + 1$ can be considered as a vector $\\begin{bmatrix}1 \\\\ -2 \\\\ 0 \\\\ 1\\end{bmatrix}$ in the basis $\\{x^3, x^2, x, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector norm\n",
    "Vectors typically provide an (approximate) description of a physical (or some other) object. \n",
    "\n",
    "One of the main question is **how accurate** the approximation is (1%, 10%). \n",
    "\n",
    "What is an acceptable representation, of course, depends on the particular applications. For example:\n",
    "- In partial differential equations accuracies $10^{-5} - 10^{-10}$ are the typical case\n",
    "- In data mining sometimes an error of $80\\%$ is ok, since the interesting signal is corrupted by a huge noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distances and norms\n",
    "Norm is a **qualitative measure of smallness of a vector** and is typically denoted as $\\Vert x \\Vert$.\n",
    "\n",
    "The norm should satisfy certain properties:\n",
    "\n",
    "- $\\Vert \\alpha x \\Vert = |\\alpha| \\Vert x \\Vert$,\n",
    "- $\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert$ (triangle inequality),\n",
    "- If $\\Vert x \\Vert = 0$ then $x = 0$.\n",
    "\n",
    "The distance between two vectors is then defined as\n",
    "$$\n",
    "   d(x, y) = \\Vert x - y \\Vert.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard norms\n",
    "The most well-known and widely used norm is **euclidean norm**:\n",
    "$$\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},$$\n",
    "which corresponds to the distance in our real life (the vectors might have complex elements, thus is the modulus here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $p$-norm\n",
    "Euclidean norm, or $2$-norm, is a subclass of an important class of $p$-norms:\n",
    "$$\n",
    " \\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n",
    "$$\n",
    "There are two very important special cases:\n",
    "- Infinity norm, or Chebyshev norm which is defined as the maximal element: $\\Vert x \\Vert_{\\infty} = \\max_i | x_i|$\n",
    "- $L_1$ norm (or **Manhattan distance**) which is defined as the sum of modules of the elements of $x$: $\\Vert x \\Vert_1 = \\sum_i |x_i|$  \n",
    "<img src=\"pics/chebyshev.jpeg\" style=\"float: left; height: 1%\">  <img src=\"pics/manhattan.jpeg\" style=\"height\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will give examples where Manhattan is very important: it all relates to the **compressed sensing** methods \n",
    "that emerged in the mid-00s as one of the most popular research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Equivalence of the norms\n",
    "All norms are equivalent in the sense that\n",
    "$$\n",
    "   C_1 \\Vert x \\Vert_* \\leq  \\Vert x \\Vert_{**} \\leq C_2 \\Vert x \\Vert_*\n",
    "$$  \n",
    "for some constants $C_1(n), C_2(n)$, $x \\in \\mathbb{R}^n$ for any pairs of norms $\\Vert \\cdot \\Vert_*$ and $\\Vert \\cdot \\Vert_{**}$. The equivalence of the norms basically means that if the vector is small in one norm, it is small in another norm. However, the constants can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Computing norms in Python\n",
    "The numpy package has all you need for computing norms (```np.linalg.norm``` function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error: 0.00244402842285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "a = np.ones(n)\n",
    "b = a + 1e-3 * np.random.randn(n)\n",
    "print 'Relative error:', np.linalg.norm(a - b, np.inf) / np.linalg.norm(b, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Unit disks in different norms\n",
    "A unit disk is a set of point such that $\\Vert x \\Vert \\leq 1$. For the Frobenius norm is a disk; for other norms the \"disks\" look different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11515c490>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsMBgjDGmh/8PW3lF3ZU/N7MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11511b2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "p = 1. #Which norm do we use\n",
    "M = 40000 #Number of sampling points\n",
    "a = np.random.randn(M, 2)\n",
    "b = []\n",
    "for i in xrange(M):\n",
    "    if np.linalg.norm(a[i, :], p) <= 1:\n",
    "        b.append(a[i, :])\n",
    "b = np.array(b)\n",
    "plt.plot(b[:, 0], b[:, 1], '.')\n",
    "plt.axis('equal')\n",
    "plt.title('Unit disk in the p-th norm, $p={0:}$'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why $L_1$-norm can be important\n",
    "$L_1$ norm, as it was discovered quite recently, plays an important role in **compressed sensing**. \n",
    "\n",
    "The simplest formulation is as follows:\n",
    "- You have some observations $f$ \n",
    "- You have a linear model $Ax = f$, where $A$ is an $n \\times m$ matrix, $A$ is **known**\n",
    "- The number of equations, $n$ is less than the number of unknowns, $m$\n",
    "\n",
    "The question: can we find the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The solution is obviously non-unique, so a natural approach is to find the solution that is minimal in the certain sense:\n",
    "\n",
    "$$ \\Vert x \\Vert \\rightarrow \\min, \\quad \\mbox{subject to } Ax = f$$\n",
    "\n",
    "Typical choice of $\\Vert x \\Vert = \\Vert x \\Vert_2$ leads to the **linear least squares problem** (and has been used for ages).  \n",
    "\n",
    "The choice $\\Vert x \\Vert = \\Vert x \\Vert_1$ leads to the [**compressed sensing**]\n",
    "\n",
    "(https://en.wikipedia.org/wiki/Compressed_sensing) and what happens, it typically yields the **sparsest solution**.  \n",
    "\n",
    "[A short demo](tv-denoising-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a stable algorithm?\n",
    "\n",
    "And we finalize the lecture by the concept of **stability**.\n",
    "\n",
    "Let $x$ be an object (for example, a vector). Let $f(x)$ be the function (functional) you want to evaluate.  \n",
    "\n",
    "You also have a **numerical algorithm** ``alg(x)`` that actually computes **approximation** to $f(x)$.  \n",
    "\n",
    "The algorithm is called **forward stable**, if $$\\Vert alg(x) - f(x) \\Vert  \\leq \\varepsilon $$  \n",
    "\n",
    "The algorithm is called **backward stable**, if for any $x$ there is a close vector $x + \\delta x$ such that\n",
    "\n",
    "$$alg(x) = f(x + \\delta x)$$\n",
    "\n",
    "and $\\Vert \\delta x \\Vert$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical example\n",
    "A classical example is the **solution of linear systems of equations** using LU-factorizations\n",
    "\n",
    "We consider the **Hilbert matrix** with the elements\n",
    "\n",
    "$$A = \\{a_{ij}\\}, \\quad a_{ij} = \\frac{1}{i + j + 1}, \\quad i,j = 0, \\ldots, n-1.$$\n",
    "\n",
    "And consider a linear system\n",
    "\n",
    "$$Ax = f.$$\n",
    "\n",
    "(We will look into matrices in more details in the next lecture, and for linear systems in the upcoming weeks, but now you actually **see** the linear system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0736826896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 500\n",
    "a = [[1.0/(i + j + 1) for i in range(n)] for j in range(n)] #Hil\n",
    "a = np.array(a)\n",
    "rhs =  np.random.random(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8802880145e-07\n"
     ]
    }
   ],
   "source": [
    "rhs =  np.ones(n)\n",
    "sol = np.linalg.solve(a, rhs)\n",
    "print np.linalg.norm(a.dot(sol) - rhs)/np.linalg.norm(rhs) #Ax - y\n",
    "#plt.plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Floating point  (double, single, number of bytes), rounding error\n",
    "- Norms are measures of smallness, used to compute the accuracy\n",
    "- $1$, $p$ and Euclidean norms \n",
    "- $L_1$ is used in compressed sensing as a surrogate for sparsity (later lectures) \n",
    "- Forward/backward error (and stability of algorithms)  (later lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
